library(tidyverse)
n <- 1000
ncol <- 50
meanoffactor1 <- c(rep(-10, times = n/4),
rep(-5, times = n/4),
rep(5, times = n/4),
rep(10, times = n/4))
meanoffactor2 <- 2*(-0.5 + seq(1:n) %% 2)
sdoffactor  <- sample(0:1,rep=T, size=n)*2+1 #either 1 or 3
df_dist <- as.data.frame(cbind(meanoffactor1,meanoffactor2,sdoffactor))
df_1f <- cbind(as.data.frame(matrix(nrow = n,ncol=ncol)),seq(1:n)) # single factor data
df_2f <- cbind(as.data.frame(matrix(nrow = n,ncol=ncol)),seq(1:n)) # two factor data
names(df_1f)[names(df_1f)=="seq(1:n)"] <- "id"
names(df_2f)[names(df_2f)=="seq(1:n)"] <- "id"
df_1f_0noise <- as.data.frame(matrix(nrow = n,ncol=ncol)) # single factor data, no noise, just mu
df_2f_0noise  <- as.data.frame(matrix(nrow = n,ncol=ncol)) # two factor data w/ no noise, just mu_(i,j)
for (row in 1:n){
for (column in 1:ncol) {
mu1 <- ((column - ncol/2)*meanoffactor1[row])/100
df_1f_0noise[row,column] <- mu1
#    df_1f[row,column] <- round(rnorm(1,mean=mu1,sd=sdoffactor[row]),digits=8)
df_1f[row,column] <- rnorm(1,mean=mu1,sd=sdoffactor[row])
mu2 <- mu1+(column %% 2)*3*meanoffactor2[row]
#    df_2f[row,column] <- round(rnorm(1,mean=mu2,sd=sdoffactor[row]),digits=8)
df_2f[row,column] <- rnorm(1,mean=mu2,sd=sdoffactor[row])
df_2f_0noise[row,column] <- mu2
}
}
View(df_1f)
gather(., variable, value)[-1,] %>%
left_join(factor_loadings, by = "variable")
resampled_dfs_2f[[1]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable")
)
factor_out2$scores
resampled_dfs_2f[[1]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable") %>%
mutate(., mr1_manual = value*MR1,
mr2_manual = value*MR2)
)
resampled_dfs_2f[[1]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable") %>%
mutate(., mr1_manual = as.numeric(value)*MR1
)
)
resampled_dfs_2f[[1]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable") %>%
mutate(., mr1_manual = as.numeric(value)*MR1,
mr2_manual = as.numeric(value)*MR2)
)
long_form <- resampled_dfs_2f[[1]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable") %>%
mutate(., mr1_manual = as.numeric(value)*MR1,
mr2_manual = as.numeric(value)*MR2) %>%
summarize(., mr1_score = sum(mr1_manual),
mr2_score = sum(mr2_manual)))
View(long_form)
View(long_form[["4"]])
View(long_form[["4"]])
for(i in 1:n) {
long_form <- resampled_dfs_2f[[i]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable") %>%
mutate(., mr1_manual = as.numeric(value)*MR1,
mr2_manual = as.numeric(value)*MR2) %>%
summarize(., mr1_score = sum(mr1_manual),
mr2_score = sum(mr2_manual)))
resampled_2fscores[[i]] <- bind_rows(long_form)
if ((i %%50)==0) { print(i)}
}
resampled_2fscores <- vector("list", length = n)
for(i in 1:n) {
long_form <- resampled_dfs_2f[[i]] %>% rownames_to_column("iter_num")  %>%
split(.$iter_num) %>%
purrr::map( ~ gather(., variable, value)[-1,] %>%
left_join(factor2_loadings, by = "variable") %>%
mutate(., mr1_manual = as.numeric(value)*MR1,
mr2_manual = as.numeric(value)*MR2) %>%
summarize(., mr1_score = sum(mr1_manual),
mr2_score = sum(mr2_manual)))
resampled_2fscores[[i]] <- bind_rows(long_form)
if ((i %%50)==0) { print(i)}
}
View(resampled_2fscores)
View(resampled_2fscores[[3]])
View(resampled_scores)
View(resampled_scores[[7]])
# Making manual scores for two factor resamples:
factor2_loadings <- data.frame(unclass(factor_out2$loadings)) %>% rownames_to_column("variable")
View(factor2_loadings)
df_2f[[i]]
df_2f[i]
# Scoring algorithm to get manual scores for each row:
factor2_loadings <- data.frame(unclass(factor_out2$loadings)) %>% rownames_to_column("variable")
View(factor2_loadings)
row_scores2f <- data.frame(factor_out2$scores) %>%
rownames_to_column("row")
View(row_scores2f)
df_2f[i,] %>% gather(variable, value)
df_2f[i,]
View(df_2f[i,])
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable")
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR3*value) %>%
summarize(mr1_score = sum(factor1_loading),
mr2_score = sum(factor2_loading))
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_score = sum(factor1_loading),
mr2_score = sum(factor2_loading))
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_score = sum(factor1_loading),
mr2_score = sum(factor2_loading)) %>% cbind(row_scores2f)
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_score = sum(factor1_loading),
mr2_score = sum(factor2_loading)) %>% cbind(row_scores2f[i,])
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_score = sum(factor1_loading),
mr2_score = sum(factor2_loading)) %>% cbind(.,row_scores2f[i,])
df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_score = sum(factor1_loading),
mr2_score = sum(factor2_loading)) %>% cbind(row_scores2f[i,],.)
for(i in 1:nrow(row_scores2f)) {
focal_row <- row_scores$row[i]
manual_row_scores2f[[i]] <- df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_manual = sum(factor1_loading),
mr2_manual = sum(factor2_loading)) %>% cbind(row_scores2f[i,],.)
}
manual_row_scores2f <- vector("list", length = nrow)row_scores2f
for(i in 1:nrow(row_scores2f)) {
focal_row <- row_scores$row[i]
manual_row_scores2f[[i]] <- df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_manual = sum(factor1_loading),
mr2_manual = sum(factor2_loading)) %>% cbind(row_scores2f[i,],.)
}
manual_row_scores2f <- vector("list", length = nrow(row_scores2f)
focal_row <- row_scores$row[i]
focal_row <- row_scores2f$row[i]
for(i in 1:nrow(row_scores2f)) {
focal_row <- row_scores2f$row[i]
manual_row_scores2f[[i]] <- df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_manual = sum(factor1_loading),
mr2_manual = sum(factor2_loading)) %>% cbind(row_scores2f[i,],.)
}
manual_row_scores2f <- vector("list", length = nrow(row_scores2f)
focal_row <- row_scores2f$row[i]
row_scores2f$row[i]
row_scores2f
row_scores2f$row[i]
for(i in 1:nrow(row_scores2f)) {
manual_row_scores2f[[i]] <- df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_manual = sum(factor1_loading),
mr2_manual = sum(factor2_loading)) %>% cbind(row_scores2f[i,],.)
}
manual_row_scores2f <- vector("list", length = nrow(row_scores2f)
manual_row_scores2f <- vector("list", length = nrow(row_scores2f))
manual_row_scores2f <- vector("list", length = nrow(row_scores2f))
for(i in 1:nrow(row_scores2f)) {
manual_row_scores2f[[i]] <- df_2f[i,] %>% gather(variable, value) %>%
left_join(factor2_loadings, by = "variable") %>% mutate(factor1_loading = MR1*value,
factor2_loading = MR2*value) %>%
summarize(mr1_manual = sum(factor1_loading),
mr2_manual = sum(factor2_loading)) %>% cbind(row_scores2f[i,],.)
}
manual_row_scores2f <- bind_rows(manual_row_scores2f)
View(manual_row_scores2f)
#TAKE THIS AND WORK INTO MAIN FORK
manual_row_scores2f %>% ggplot(aes(x=MR1,y=factor1_loading)) + geom_point()
#TAKE THIS AND WORK INTO MAIN FORK
manual_row_scores2f %>% ggplot(aes(x=MR1,y=mr1_manual)) + geom_point()
#TAKE THIS AND WORK INTO MAIN FORK
manual_row_scores2f %>% ggplot(aes(x=MR2,y=mr2_manual)) + geom_point()
View(factor2_loadings)
View(row_scores2f)
#TAKE THIS AND WORK INTO MAIN FORK
manual_row_scores2f %>% ggplot(aes(x=MR1,y=mr1_manual)) + geom_point()
manual_row_scores2f %>%
ggplot(aes(x=MR2,y=mr2_manual)) +
geom_point()
MRmodel <- lm(data=manual_row_scores2f,mr1_manual~MR1)
lm(data=manual_row_scores2f,mr1_manual~MR1)
MRmodel2 <- lm(data=manual_row_scores2f,mr1_manual~MR1)
print(summary(MRmodel2)) #note, there's an intercept
MRmodel$coefficients[1]
MRmodel2_fac1 <- lm(data=manual_row_scores2f,mr1_manual~MR1)
print(summary(MRmodel2_fac1)) #note, there's an intercept
#row_scores$manualscaled <- row_scores$manual_score/MRmodel$coefficients[2]
manual_row_scores2f$manualscaled_mr1 <- (manual_row_scores2f$mr1_manual-MRmodel2_fac1$coefficients[1])/MRmodel2_fac1$coefficients[2]
manual_row_scores2f %>% ggplot(aes(x=MR1,y=manualscaled_mr1)) + geom_point()
manual_row_scores2f$manualscaled_mr2 <- (manual_row_scores2f$mr2_manual-MRmodel2_fac2$coefficients[1])/MRmodel2_fac2$coefficients[2]
MRmodel2_fac2 <- lm(data=manual_row_scores2f,mr2_manual~MR2) # Scale for Factor 2
print(summary(MRmodel2_fac2)) #note, there's an intercept
manual_row_scores2f$manualscaled_mr2 <- (manual_row_scores2f$mr2_manual-MRmodel2_fac2$coefficients[1])/MRmodel2_fac2$coefficients[2]
manual_row_scores2f %>% ggplot(aes(x=MR1,y=manualscaled_mr2)) + geom_point()
manual_row_scores2f %>% ggplot(aes(x=MR2,y=manualscaled_mr2)) + geom_point()
?fa
factor.scores(df_1f)
psych::factor.scores(df_1f)
psych::factor.scores(df_1f, factor_out)
single_factor_scores <- psych::factor.score(df_1f, factor_out)
single_factor_scores <- psych::factor.scores(df_1f, factor_out)
single_factor_scores$weights
cor(df_1f)
corr_1f <- cor(df_1f)
factor_out$loadings
factor_loadings <- data.frame(unclass(factor_out$loadings))
factor_loadings
w <- corr_1f_R^-1
w <- corr_1f^-1
w <- corr_1f^-1*factor_loadings
View(w)
single_factor_scores$weights # lets try to replicate these...
single_factor_scores <- psych::factor.scores(df_1f, factor_out, method = c"Thurstone")
single_factor_scores <- psych::factor.scores(df_1f, factor_out, method = "Thurstone")
single_factor_scores$weights # lets try to replicate these...
?solve
"Thurstone" = { w <- try(solve(r,S),silent=TRUE )  #these are the factor weights (see Grice eq. 5)
if(class(w)=="try-error") {message("In factor.scores, the correlation matrix is singular, an approximation is used")
r <- cor.smooth(r)}
w <- try(solve(r,S),silent=TRUE)
if(class(w)=="try-error") {message("I was unable to calculate the factor score weights, factor loadings used instead")
w <- f}
colnames(w) <- colnames(f)
rownames(w) <- rownames(f)
},
S <- factor_loadings %*% Phi
factor_out$phi
r <- cor(df_1f,use="pairwise")
w <- try(solve(r,S),silent=TRUE )
S <- factor_loadings %*% Phi
S <- factor_loadings %*% factor_out$phi
nf <- dim(factor_out)[2]
nf <- dim(factor_out2)[2]
diag(1,nf,nf)
nf <- dim(factor_out2)[2]
diag(1,nf,nf)
factor_out2$phi
# Tinkering with psych source code
single_factor_scores <- psych::factor.scores(df_1f, factor_out, method = "Thurstone")
View(single_factor_scores)
single_factor_scores[["scores"]]
factor_out$scores
single_factor_scores[["scores"]]
factor_out$scores
single_factor_scores$weights
w <- (corr_1f^-1)*factor_loadings
single_factor_scores$weights # lets try to replicate these...
w
w*factor_loadings
corr_1f
corr_1f <- cor(df_1f, "pairwaise")
?cor.smooth
corr_1f <- cor(df_1f")
corr_1f <- cor(df_1f)
View(corr_1f)
View(corr_1f)
?cor
r <- cor(df_1f,use="pairwise")
View(r)
f <- factor_out
ncol(f)
!is.matrix(f)
Phi <- f$Phi
f <- loadings(f)
ncol(f)==1
nf <- dim(f)[2]
Phi <- diag(1,nf,nf)
dim(df_1f)[1] == dim(f)[1]
r <- cor(x,use="pairwise")
r <- cor(df_1f,use="pairwise")
S <- f %*% Phi
w <- try(solve(r,S))
View(w)
View(S)
dim(f)[2]
dim(f)
?diag
View(Phi)
View(S)
View(factor_loadings)
?solve
View(w)
View(single_factor_scores$weights) # lets try to replicate these...
w <- try(solve(r,f))
View(w)
is.null(w)
R2 <- diag(t(w) %*% S)
df_1f_mat <- data.matrix(df_1f)
which(is.na(df_1f_mat),arr.ind=TRUE)
scale(df_1f_mat)
scale(df_1f_mat) %*% w
scores <- scale(df_1f_mat) %*% w
?psych.predict
?psych::psych.predict
?psych::predict.psych
resampled_scores[[1]]
resampled_dfs[[1]]
predict.psych(factor_out, resampled_dfs[[1]], old.data = df_1f)
psych::predict.psych(factor_out, resampled_dfs[[1]], old.data = df_1f)
scale(resampled_dfs[[1]]) %*% w
scale(data.matrix(resampled_dfs[[1]])) %*% w
data.matrix(resampled_dfs[[1]])
w
newscores <- scale(data.matrix(resampled_dfs[[1]])) %*% w
w
scale(df_1f_mat) %*% w
cale(df_1f_mat[1:10,]) %*% w
scale(df_1f_mat[1:10,]) %*% w
resampled_dfs[[1]]
df_1f_mat
new_data <- data.matrix(resampled_dfs[[1]])
newscores <- scale(new_data) %*% w
View(new_data)
rm(list = /ls())
rm(list = ls())
install.packages("rtweet")
install.packages("maps")
for (i in 1:24){
#Search for 50 recent tweets about computational social science
css_tweets<-search_tweets("Computational Social Science", n=50, include_rts = FALSE)
#Randomly pick one of them, which appears in the `text` variable with the `css_tweets` dataframe
lucky_tweet<-sample(css_tweets$text, 1)
post_tweet(lucky_tweet)
Sys.sleep(3600)
#3600 seconds is 60 minutes
}
install.packages("rvest")
library(rvest)
wikipedia_page<-read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
wikipedia_page
section_of_wikipedia<-html_node(wikipedia_page, xpath='//*[@id="mw-content-text"]/div/table')
head(section_of_wikipedia)
section_of_wikipedia<-html_node(wikipedia_page, xpath="//*[@id="mw-content-text"]/div/table[2]")
section_of_wikipedia<-html_node(wikipedia_page, xpath='//*[@id="mw-content-text"]/div/table[2]')
head(section_of_wikipedia)
section_of_wikipedia<-html_node(wikipedia_page, xpath='//*[@id="mw-content-text"]/div/table[2]')
head(section_of_wikipedia)
section_of_wikipedia<-html_node(wikipedia_page, xpath='//*[@id="mw-content-text"]/div/table[2]/tbody')
head(section_of_wikipedia)
section_of_wikipedia<-html_node(wikipedia_page, xpath='//*[@id="mw-content-text"]/div/table[2]/tbody')
head(section_of_wikipedia)
health_rankings<-html_table(section_of_wikipedia)
head(health_rankings[,(1:2)])
duke_page<-read_html("https://www.duke.edu")
duke_events<-html_nodes(duke_page, css="li:nth-child(1) .epsilon")
html_text(duke_events)
install.packages("tidytext")
duke_web_scrape<- "Class of 2018: Senior Stories of Discovery, Learning and Serving\n\n\t\t\t\t\t\t\t"
grepl("Class", duke_web_scrape)
gsub("\t", "", duke_web_scrape)
gsub("\t|\n", "", duke_web_scrape)
some_text<-c("This","Professor","is","not","so","great")
some_text[grep("^[P]", some_text)]
text_chunk<-c("[This Professor is not so Great]")
gsub("\","", text_chunk)
text_chunk<-c("[This Professor is not so Great]")
gsub('\\[|\\]',"", text_chunk)
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
head(trumptweets$text)
install.packages("tm")
library(tm)
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
trump_corpus
install.packages("tidytext")
library(tidytext)
library(dplyr)
tidy_trump_tweets<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
trump_corpus <- tm_map(trump_corpus, removeWords, stopwords("english"))
data("stop_words")
tidy_trump_tweets<-tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
library(tidytext)
library(dplyr)
tidy_trump_tweets<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text)
data("stop_words")
trump_tweet_top_words<-
tidy_trump_tweets %>%
anti_join(stop_words) %>%
count(word) %>%
arrange(desc(n))
trump_tweet_top_words<-
trump_tweet_top_words[-grep("https|t.co|amp|rt",
trump_tweet_top_words$word),]
#select only top words
top_20<-trump_tweet_top_words[1:20,]
#create factor variable to sort by frequency
trump_tweet_top_words$word <- factor(trump_tweet_top_words$word, levels = trump_tweet_top_words$word[order(trump_tweet_top_words$n,decreasing=TRUE)])
library(ggplot2)
ggplot(top_20, aes(x=word, y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Trump's Tweets")+
xlab("")+
guides(fill=FALSE)
#select only top words
top_20<-trump_tweet_top_words[1:20,]
#create factor variable to sort by frequency
trump_tweet_top_words$word <- factor(trump_tweet_top_words$word, levels = trump_tweet_top_words$word[order(trump_tweet_top_words$n,decreasing=TRUE)])
library(ggplot2)
ggplot(top_20, aes(x=word, y=n)+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Trump's Tweets")+
xlab("")+
guides(fill=FALSE)
ggplot(top_20, aes(x=word, y=n))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Trump's Tweets")+
xlab("")+
guides(fill=FALSE)
tidy_trump_tfidf<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text) %>%
anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
top_tfidf<-tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf$word[1]
economic_dictionary<-c("economy","unemployment","trade","tariffs")
library(stringr)
economic_tweets<-trumptweets[str_detect(trumptweets$text, paste(economic_dictionary, collapse="|")),]
head(get_sentiments("afinn"))
head(get_sentiments("afinn"))
?get_sentiments
head(get_sentiments(lexicon = "afinn"))
trump_tweet_sentiment <- tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(created_at, sentiment)
head(trump_tweet_sentiment)
tidy_trump_tweets$date<-as.Date(tidy_trump_tweets$created_at,
format="%Y-%m-%d %x")
trump_sentiment_plot <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
filter(sentiment=="negative") %>%
count(date, sentiment)
library(ggplot2)
ggplot(trump_sentiment_plot, aes(x=date, y=n))+
geom_line(color="red")+
theme_minimal()+
ylab("Frequency of Negative Words in Trump's Tweets")+
xlab("Date")
trump_approval<-read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")
trump_approval$date<-as.Date(trump_approval$modeldate, format="%m/%d/%Y")
approval_plot<-
trump_approval %>%
filter(subgroup=="Adults") %>%
filter(date>min(trump_sentiment_plot$date)) %>%
group_by(date) %>%
summarise(approval=mean(approve_estimate))
#plot
ggplot(approval_plot, aes(x=date, y=approval))+
geom_line(group=1)+
theme_minimal()+
ylab("% of American Adults who Approve of Trump")+
xlab("Date")
library(devtools)
install_github("cbail/textnets")
library(textnets)
data("sotu")
sotu_first_speeches <- sotu %>% group_by(president) %>% slice(1L)
prepped_sotu <- PrepText(sotu_first_speeches, groupvar = "president", textvar = "sotu_text", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
library(textnets)
data("sotu")
sotu_first_speeches <- sotu %>% group_by(president) %>% slice(1L)
prepped_sotu <- PrepText(sotu_first_speeches, groupvar = "president", textvar = "sotu_text", node_type = "groups", tokenizer = "words", pos = "nouns", remove_stop_words = TRUE, compound_nouns = TRUE)
